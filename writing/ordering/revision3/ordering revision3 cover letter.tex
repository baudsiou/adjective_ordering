\documentclass[12pt]{article}
\usepackage[hmargin={1in},vmargin={1in,1in},foot={.6in}]{geometry}   
\geometry{letterpaper}              
\usepackage{color,graphicx}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{varioref}
\usepackage{textcomp}
\usepackage{textcomp}
\usepackage{mflogo}
\usepackage{wasysym}
\usepackage[normalem]{ulem}
\usepackage{hyperref}

\newcommand{\HRule}{\rule{\linewidth}{0.25mm}}

\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{plain} % options: empty , plain , fancy
\lhead{}\chead{}\rhead{}
\renewcommand{\headrulewidth}{.5pt}
\lfoot{}\cfoot{\thepage}\rfoot{}
\newcommand{\txtp}{\textipa}
\renewcommand{\rm}{\textrm}
\newcommand{\sem}[1]{\mbox{$[\![$#1$]\!]$}}
\newcommand{\lam}{$\lambda$}
\newcommand{\lan}{$\langle$}
\newcommand{\ran}{$\rangle$}
\newcommand{\type}[1]{\ensuremath{\left \langle #1 \right \rangle }}

\newcommand{\bex}{\begin{exe}}
\newcommand{\eex}{\end{exe}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}

\newcommand{\gcs}[1]{\textcolor{blue}{[gcs: #1]}}
\definecolor{Green}{RGB}{10,200,100}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}
\newcommand{\jd}[1]{\textcolor{red}{[jd: #1]}}

%\thispagestyle{empty}

\begin{document}

{\flushright

\vspace{25pt}
T\"{u}bingen, Germany\\[20pt]

\noindent November 12, 2018\\[25pt]}


\noindent Dear Kjell Johan,\\

\noindent We would like to thank you yet again for your comments on our revised paper, ``On the grammatical source of adjective ordering preferences.'' We were sorry to hear that you found our revision to have moved in the wrong direction, but we are grateful for the additional chance to address the issues that remain. As you will recall, you identified a relatively clear path to publication on the basis of addressing the following concerns:

\ben

\item \emph{Now for one thing, it is not clear that the decision to remove the
	discussion of the probability of no misclassification is wise. In
	particular, the probability that all and only the objects that are in fact,
	say, brown and small boxes are, in response to "small brown box" versus
	``brown small box'', judged to be brown and small boxes can be regarded as
	just as important as the notion of successful reference resolution or
	correct classification of the intended referent that you now characterize.
	Besides, as it appeared from the previous version, relatively simple
	reasoning is sufficient to show that the probability of classifying, say,
	all three boxes where one is brown and small, one is brown and not small and
	one is small and not brown correctly with respect to being brown and small
	is higher when ``brown'' is processed first, without making the
	misclassification potential proportional to the cardinality of the set of
	objects. This argument would be relatively reader-accessible, and I
	recommend that you include it if you still consider it valid.}

We have added back in the discussion of the probability of no misclassifications.
	
%\item \emph{Secondly, the assumption about the fixed processing budget and the concrete
%	example of how ordering according to subjectivity maximizes the probability
%	of successful referent resolution are now presented in a separate subsection
%	which is very difficult to understand, not because of the amount or the
%	level of mathematics, but because notations and formulations are unclear and
%	there are inconsistencies and errors.}
%
%XXX
	
\item \emph{Let me first comment on your statement just above the subsection 4.1 where
	you advise readers ``who are already convinced by our argument or wary of
	math'' to skip it: readers could not be ``already convinced'' because your
	argument has not yet been presented at this point, either formally or
	informally.}

We have removed this passage.
	
%\item \emph{You present a scenario where there is exactly one small brown box (sb),
%	exactly one brown box which is not small (b) and exactly one small box which
%	is not brown (s), and compute, roughly, on the one hand, the probability
%	that sb is judged to be brown relative to the set of boxes and judged to be
%	small relative to the set of brown boxes, and on the other hand, the
%	probability that sb is judged to be small relative to the set of boxes and
%	judged to be brown relative to the set of small boxes (although it is
%	somewhat more complex than this). As desired, the former is slightly higher
%	than the latter (.32 vs.~.3024).}
%
%XXX 
	
\item \emph{Note, however, that the probability that sb is judged to be the unique box
	that is brown and small is not captured, that is, the probability that
	neither b nor s is judged to be brown and small is not taken into account,
	thus the paraphrase ``successful (intended) referent resolution'' is somewhat
	misleading.}

We have revised our language to be more careful and precise. We now talk of the probability of correctly classifying the intended referent. This value has a large influence on the probability of successful reference resolution (successful referent resolution cannot happen without successful referent classification), but the two are indeed not identical.
	
\item \emph{Now before that, you introduce `ref'. It is unclear what `ref' is in the
	definitions (3) and (4). It is paraphrased as ``the intended referent(s)'',
	but it must be something like a bound variable in an implicit universal
	quantification, so that} 
	
	`$p_{adj}(ref, NP) = 1 -  \varepsilon_{adj}(|NP|)$'
	
	\emph{must be read as something like}
	
	\emph{`for any object ref and any adjective adj and any set of objects NP, the
	probability that in response to adj, ref is judged to have the property
	expressed by adj relative to NP if ref in fact has (or is meant to have)
	that property, and is judged to not have the property expressed by adj
	relative to NP if ref in fact does not have (or is not meant to have) that
	property, equals 1 - the misclassification potential of adj as a function of
	the cardinality of NP'.}

We have modified the introduction of these equations in an attempt to limit confusion. We now introduce the probability of correctly classifying some object \texttt{obj} (i.e., quantifying over potential objects), then narrow in on the probability of correctly classifying the intended referent(s) \texttt{ref}.
	
\item \emph{There are also some more minor inconsistencies and errors: in the tables
	(12) and (13), the sums do not in fact add up, as you have omitted a column
	of final products, and in (11), the misclassification potential function has
	a different type of argument from elsewhere.}

We have corrected these errors.
	
\item \emph{Now while the issues I have noted so far can probaby be remedied in a
	relatively straightforward way, there is a further problem which requires
	more thought. Your comparison between ``small brown box'' and ``brown small
	box'' regarding correct classification of the box which is in fact or meant
	to be both brown and small is favorable for the former in your toy example
	and more generally as long as all else is equal regarding the properties
	expressed by the adjectives in the set under consideration and the
	misclassification potentials for the two adjectives are sufficiently far
	apart. But as soon as the scenario is changed so that, say, there are four
	boxes, one brown and small, two  brown and not small, and one small and not
	brown, my calculation results in a slightly higher probability of correct
	classification of sb under the ``brown small box'' condition (.511376) than
	under the ``small brown box'' condition (.508896) provided the
	misclassification potential of "small" ranges from 26 to 38 relative to a
	set of 1 to 4 and that of "brown" ranges from 12 to 24 relative to a set of
	1 to 4. As far as I can see, this  also means that your theorem (14) is not
	valid in generality but would need to be qualified. It is essential that
	this problem is sorted out.}

We are grateful to you for pointing out the limits of the generality of our claim. This realization prompted us to systematically explore the parameter space. We now report on this exploration and our findings, and we link to a web-based appendix with runnable code that walks the reader through the analysis. As we state now in our revision, of the 100,000+ cases we explored, 93\% were in line with the predictions of misclassification minimization via  subjectivity-based ordering. This finding had led us to discuss an issue we had been mulling over but didn't feel comfortable raising until now: whether ordering preferences derive from  online subjectivity-based reasoning, or whether the preferences reflect regularities in our input that get strengthened as language evolves.
	
	
\een

Thank you again for the thorough and thoughtful comments on our work. We hope that you will like the new version of the paper. Please let us know if you require additional information. We look forward to hearing from you!\\[25pt]


\noindent Yours sincerely,\\[10pt]

\noindent Gregory Scontras, Judith Degen, and Noah D.~Goodman

\newpage





\end{document}














